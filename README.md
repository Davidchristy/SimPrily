Created by Ariella Gladstein, based on code from Consuelo Quinto Cortes and Krishna Veeramah.
Also worked on by David Christy, Logan Gantner, and Mack Skodiak.  
agladstein@email.arizona.edu

## About
SimPrily runs genome simulations with user defined parameters or parameters randomly generated by priors and computes genomic statistics on the simulation output.  
Version 1

1. Run genome simulation with model defined by prior distributions of parameters and demographic model structure.
2. Take into account SNP array ascertainment bias by creating pseudo array based on priors of number of samples of discovery populations and allele frequency cut-off.
3. Calculate genomic summary statistics on simulated genomes and pseudo arrays. 

This is ideal for use with Approximate Bayesian Computation on whole genome or SNP array data.

Uses c++ programs macs and GERMLINE. For more information on these programs, see:  
https://github.com/gchen98/macs  
https://github.com/sgusev/GERMLINE  

## Install

cd to the directory you want to work in,
```bash
git clone https://github.com/agladstein/SimPrily.git
```

#### Environment Set up
If using Vagrant (this is recommended if running on non-Linux OS):

```bash
vagrant up
vagrant ssh
``` 

```bash
sudo apt-get update
sudo apt-get upgrade
sudo apt-get install python-virtualenv git python-dev
sudo easy_install -U distribute
cd ~
virtualenv simprily_env
source ~/simprily_env/bin/activate
pip install --upgrade pip
pip install pip-tools
cd /vagrant
pip-sync
~/simprily_env/bin/python simprily.py examples/eg1/param_file_eg1.txt examples/eg1/model_file_eg1.csv macs 1 array_template/ill_650_test.bed 1 False out_dir
```

If not using Vagrant:
```bash
sudo apt-get update
sudo apt-get upgrade
sudo apt-get install python-virtualenv git python-dev
sudo easy_install -U distribute
virtualenv simprily_env
pip install --upgrade pip
pip install pip-tools
pip-sync
simprily_env/bin/python simprily.py examples/eg2/param_file_eg2.txt examples/eg2/model_file_eg2.csv macs 1 array_template/ill_650_test.bed 0 True output_dir
```


## Usage

e.g. One Test simulation:  
```
python simprily.py examples/eg1/param_file_eg1.txt examples/eg1/model_file_eg1.csv macs 1 array_template/ill_650_test.bed 1 False out_dir
```

#### Input  
`simprily.py` takes 8 arguments.   

Run as  
```
python simprily.py param_file.txt model_file.csv sim_option jobID array_template germline output_dir
```
1. `param_file.txt` = full path to file containing parameter values or priors
2. `model_file.csv` = full path to file containing model commands
3. `sim_option` = macs or macsswig
4. `jobID` = can be any unique value to identify the output  
5. `array_template` = bed file with physical position of SNPs on array to use as template for simulated pseudo array.  
6. `germline` = 0 to run GERMLINE, 1 to not run GERMLINE. ##Change this flag  
7. `random_discovery` = True to randomly pick number of individuals for SNP discovery, or False to use half of the discovery individuals.
8. `output_dir` = path to the directory to output to. No argument will use the default of current dir `.` 


#### Output
Four subdirectories are created in the directory specified in the `output_dir` argument.  
```
output_dir/sim_values
output_dir/results_sims
output_dir/sim_data
output_dir/germline_out
```

##### Intermediate files
Intermediate files go to `output_dir/sim_data` and `output_dir/germline_out`.    
`output_dir/sim_data` contains PLINK formated .ped and .map files created from the pseudo array, which are necessary to run GERMLINE.  
`output_dir/germline_out` contains the GERMLINE .match output and .log. The .match contains all of the identified IBD segments.  
These files are NOT automatically removed in python script, but are unnecessary once the job is complete.  

##### Results files
Output files go to `output_dir/sim_values` and `output_dir/results_sims`.  
`output_dir/sim_values` contains the parameter values used in the simulation.
The first line is a header with the parameter names.
The second line is the parameter values.  
`output_dir/results_sims` contains the summary statistics calculated from the simulation.
The first line is a header with the summary statistics names.
The second line is the summary statistics values.



-------------------------


## Running as a Workflow on Open Science Grid

The workflow creates a Python virtual environment according to the requirements.in file, and then tars up this whole
directory. That tarball is then shipped with the jobs, with the result being that wherever the jobs start up, they 
still have access to any files currently in this directory. However, note that paths will be different, so only
reference files using relative paths.


### Submitting the Workflow

Figure out what the start job id and end job id you want to have in the run. These are provided by the user as a
convenience so that outputs from multiple runs uses unique names. To submit a new workflow, run:

    ./submit [modell] [startid] [endid]

For exaple, to run a small test workflow:

    ./submit run_sims_AJmodel2_chr1_all.py 1 100

The output of the command is something similar to:

    An outputs directory will be created within the base of the workflow directory.
    Directory: /local-scratch/rynge/workflows/macsswig_simsaj_1489465382/outputs

    2017.03.13 23:23:39.930 CDT:    
    2017.03.13 23:23:39.936 CDT:   ----------------------------------------------------------------------- 
    2017.03.13 23:23:39.941 CDT:   File for submitting this DAG to Condor           : macsswig_simsaj-0.dag.condor.sub 
    2017.03.13 23:23:39.947 CDT:   Log of DAGMan debugging messages                 : macsswig_simsaj-0.dag.dagman.out 
    2017.03.13 23:23:39.952 CDT:   Log of Condor library output                     : macsswig_simsaj-0.dag.lib.out 
    2017.03.13 23:23:39.958 CDT:   Log of Condor library error messages             : macsswig_simsaj-0.dag.lib.err 
    2017.03.13 23:23:39.963 CDT:   Log of the life of condor_dagman itself          : macsswig_simsaj-0.dag.dagman.log 
    2017.03.13 23:23:39.968 CDT:    
    2017.03.13 23:23:39.984 CDT:   ----------------------------------------------------------------------- 
    2017.03.13 23:23:41.049 CDT:   Your database is compatible with Pegasus version: 4.7.3 
    2017.03.13 23:23:41.178 CDT:   Submitting to condor macsswig_simsaj-0.dag.condor.sub 
    2017.03.13 23:23:41.360 CDT:   Submitting job(s). 
    2017.03.13 23:23:41.365 CDT:   1 job(s) submitted to cluster 3891204. 
    2017.03.13 23:23:41.371 CDT:    
    2017.03.13 23:23:41.376 CDT:   Your workflow has been started and is running in the base directory: 
    2017.03.13 23:23:41.381 CDT:    
    2017.03.13 23:23:41.387 CDT:     /local-scratch/rynge/workflows/macsswig_simsaj_1489465382/workflow/macsswig_simsaj_1489465382 
    2017.03.13 23:23:41.392 CDT:    
    2017.03.13 23:23:41.397 CDT:   *** To monitor the workflow you can run *** 
    2017.03.13 23:23:41.403 CDT:    
    2017.03.13 23:23:41.408 CDT:     pegasus-status -l /local-scratch/rynge/workflows/macsswig_simsaj_1489465382/workflow/macsswig_simsaj_1489465382 
    2017.03.13 23:23:41.413 CDT:    
    2017.03.13 23:23:41.419 CDT:   *** To remove your workflow run *** 
    2017.03.13 23:23:41.424 CDT:    
    2017.03.13 23:23:41.429 CDT:     pegasus-remove /local-scratch/rynge/workflows/macsswig_simsaj_1489465382/workflow/macsswig_simsaj_1489465382 
    2017.03.13 23:23:41.435 CDT:    
    2017.03.13 23:23:41.624 CDT:   Time taken to execute is 3.636 seconds 

Note how Pegasus uses a directory as "handle" to the workflow. This directory path can be used with various Pegasus commands.


### Monitoring

The system will send email notifications when the workflow changes state, but if you want to see the current state, use the
`pegasus-status` command. For example:

    $ pegasus-status -l /local-scratch/rynge/workflows/macsswig_simsaj_1489465382/workflow/macsswig_simsaj_1489465382
    STAT  IN_STATE  JOB                                                                                                                
    Run      03:12  macsswig_simsaj-0 ( /local-scratch/rynge/workflows/macsswig_simsaj_1489465382/workflow/macsswig_simsaj_1489465382 )
    Idle     02:31   ┣━run-sim.sh_ID0000090                                                                                            
    ...
    Idle     00:40   ┗━run-sim.sh_ID0000077                                                                                            
    Summary: 101 Condor jobs total (I:74 R:27)
    
    UNRDY READY   PRE  IN_Q  POST  DONE  FAIL %DONE STATE   DAGNAME                                 
        6     0     0   100     0     3     0   2.8 Running *macsswig_simsaj-0.dag  

The last couple of lines will tell you the overall state. Note that jobs can be "READY" but not yet submitted to the queue. The
reason for this is that the workflow is configured to keep at most 1,000 idle jobs in the queue, in order to not overwhelm the
scheduler.


### Statistics / Debugging

For successful workflows, you can generate statistics such as cumulative runtimes using the `pegasus-statistics -s all [dir]`
command. For failed workflows, `pegasus-analyzer [dir]` can help pinpoint the failures.


### Stopping / Restarting

If you want to stop a workflow, use the `pegasus-remove [dir]` command. Workflows which have stopped for some reason (removed
by the user or maybe from a longer system outage), can be restarted from where they left of with the `pegasus-run [dir]`
command.


### Links to documentation

 * [Pegasus Command Line Interface](https://pegasus.isi.edu/documentation/cli.php)
 * [Pegasus User Guide](https://pegasus.isi.edu/documentation/)
 * [OSG Connect Documentation](https://support.opensciencegrid.org/solution/categories)


-------------------------


## Using ABC with ABCestimator

### Obtaining and compiling the code
ABCtoolbox is available from  
https://bitbucket.org/phaentu/abctoolbox-public/overview  

To download  
`git clone https://bitbucket.org/phaentu/abctoolbox-public.git`  

If openMP is installed, some functions inside ABCtoolbox, including findStatsModelchoice can be parallelized. Compile as follows  
`g++ -O3 -o ABCtoolbox *.cpp -DUSE_OMP -fopenmp`  
See [openMp forum](http://forum.openmp.org/forum/viewtopic.php?f=3&t=1993&p=7809#p7809) 

If openMP is not installed, compile as follows:  
`g++ -O3 -o ABCtoolbox *.cpp`

### Removing and keeping summary statistics   
To remove summary statistics or keep summary statistics from ABCtoolbox input use the scripts  
`main_subset_sim.py` and
`main_subset_real.py`  

There are two options:    
1. Remove highly correlated statistics from ABCtoolbox input.   
The first input file should be the log file after running ABCestimator with the option pruneCorrelatedStats.  
2. Keep parameter values and statistics with high power from ABCtoolbox input.   
The first input file should be the output file *greedySearchForBestStatisticsForModelChoice.txt after running ABCestimator with the option findStatsModelChoice  

The arguments are    
- ABCtoolbox output to get summary staistics from  
- ABCtoolbox input (id parameters statistics)  
- keep or remove  
- if using keep option, number of sets of summary statistics  

Run as  
`subset_stats/main_subset_sim.py ABC_searchStatsForModelChoice_OSG_50000_100greedySearchForBestStatisticsForModelChoice.txt input_ABCtoolbox_M1_8.txt keep 4`  
or  
`subset_stats/main_subset_sim.py ABC_estimate_OSG_100_50000_100.log input_ABCtoolbox_M1_8.txt remove`

-------------------------

## To-Do
* Include option flags instead of list arguments
* Allow for different prior distributions
* Full genome simulation pipeline (option for multi or sequential processing?)
* Provide option for additional simulators

## Known Issues
* Seed with macsswig is unstable. 
* If exponential growth is large, macs simulation will not finish. (This is a macs bug).
* If the same id is used with the same output dir as a previous run, the .map file will be appended to.